{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2d9e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Tuple\n",
    "import pandas as pd, numpy as np\n",
    "import src.commons.tools as data_tools\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "cat_cols = ['Desc_Cargo', 'Proyecto', 'genero', 'id_tipo_contrato', 'id_estado_civil', 'id_turno']\n",
    "\n",
    "def read_data(prefix: str='') -> Tuple[pd.DataFrame]:\n",
    "    training_set = os.path.join(data_tools.output_path, 'databases', f'{prefix}_dane_enriched_db.csv')\n",
    "    dane_enriched = pd.read_csv(\n",
    "        training_set,\n",
    "        parse_dates=['fecha_ingreso', 'fecha_final', 'fecha_retiro', 'fecha_nacimiento']\n",
    "    )\n",
    "    dane_dict = pd.read_excel(\n",
    "        os.path.join(data_tools.input_path, 'DICCIONARIO_DATOS_DANE.xlsx'),\n",
    "        sheet_name='MGN_ANM_MANZANA',\n",
    "        skiprows=6\n",
    "    )\n",
    "    business_dict = pd.read_excel(\n",
    "        os.path.join(data_tools.input_path, 'DICCIONARIO 1.xlsx'),\n",
    "        sheet_name='DICCIONARIO FINAL',\n",
    "        skiprows=3\n",
    "    ).drop('Unnamed: 0', axis=1)\n",
    "    return dane_enriched, dane_dict, business_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eba47631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     removing outliers...\n",
      "     getting dummies...\n",
      "     generating categorical db (quantils)...\n",
      "     saving datasets...\n"
     ]
    }
   ],
   "source": [
    "def read_data(file_path: str) -> pd.DataFrame:\n",
    "    dataset = pd.read_csv(\n",
    "            file_path,\n",
    "            parse_dates=['fecha_nacimiento', 'fecha_ingreso', 'fecha_retiro']\n",
    "        )\n",
    "    return dataset\n",
    "\n",
    "def drop_unvariant_cols(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    cat_vars = cat_cols.copy()\n",
    "    cat_vars.append('causa_retiro')\n",
    "    dataset_cats = dataset[cat_vars]\n",
    "    dataset_num = dataset.drop(cat_vars, axis=1)\n",
    "    corr = dataset_num.corr()\n",
    "    drop_cols = corr[corr.isna().all()].index\n",
    "    dataset_num = dataset_num.drop(drop_cols, axis=1)\n",
    "    dataset_ = dataset_cats.join(dataset_num)\n",
    "    return dataset_\n",
    "\n",
    "def predictive_base_processing(prefix: str=''):\n",
    "    dataset = read_data(prefix)\n",
    "    dataset_ = data_tools.years_computing(dataset)\n",
    "    #computing permanence contract time\n",
    "    dataset_['permanencia'] = (dataset_['fecha_retiro']-dataset_['fecha_ingreso']).dt.days.astype(int)\n",
    "    dataset_.loc[dataset_.permanencia<=0, 'permanencia'] = np.nan\n",
    "    dataset_ = data_tools.input_numeric_col(dataset_, 'permanencia').drop(['fecha_ingreso', 'fecha_retiro'], axis=1)\n",
    "    #removing outliers\n",
    "    print('     removing outliers...')\n",
    "    dataset_ = data_tools.outliers_remotion(dataset_)\n",
    "    cat_vars = cat_cols.copy()\n",
    "    cat_vars.append('causa_retiro')\n",
    "    dataset_cats = dataset_[cat_vars]\n",
    "    dataset_num = dataset_.drop(cat_vars, axis=1)\n",
    "    dataset_num = data_tools.feature_dane(dataset_num)\n",
    "    dataset_ = dataset_num.join(dataset_cats)\n",
    "    dataset_ = drop_unvariant_cols(dataset_)\n",
    "    return dataset_\n",
    "\n",
    "def numeric_binner(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    cat_vars = cat_cols.copy()\n",
    "    cat_vars.append('causa_retiro')\n",
    "    dataset_cats = dataset[cat_vars]\n",
    "    dataset_num = dataset.drop(cat_vars, axis=1)\n",
    "    quartils = [0, .25, .5, .75, 1.]\n",
    "    labels = ['Q1', 'Q2', 'Q3', 'Q4']\n",
    "    for col in dataset_num.columns:\n",
    "        aux = pd.qcut(x=dataset_num[col].to_numpy(), q=quartils, labels=False, duplicates=\"drop\")\n",
    "        dataset_num[col] = pd.qcut(dataset_num[col], q=4, labels=labels[:len(np.unique(aux))], duplicates=\"drop\")\n",
    "    dataset_ = dataset_num.join(dataset_cats)\n",
    "    return dataset_\n",
    "\n",
    "def save_data(dataset_cluster: pd.DataFrame, categorical_db: pd.DataFrame, file_path: str, prefix: str='') -> None:\n",
    "    dataset_cluster.to_csv(\n",
    "        os.path.join(os.path.dirname(file_path), f'{prefix}_description_numeric.csv'),\n",
    "        index=0\n",
    "    )\n",
    "    categorical_db.to_csv(\n",
    "        os.path.join(os.path.dirname(file_path), f'{prefix}_description_categorical.csv'),\n",
    "        index=0\n",
    "    )\n",
    "\n",
    "def process_predictive_sets(prefix: str='') -> None:\n",
    "    prefix = 'test'\n",
    "    file_path = os.path.join(data_tools.output_path, 'descriptive_mining', f'{prefix}_descriptive_without_featuring.csv')\n",
    "    dataset = predictive_base_processing(file_path)\n",
    "    print('     getting dummies...')\n",
    "    dataset_cluster = data_tools.get_dummies(dataset, cat_cols)\n",
    "    dataset_cluster = dataset_cluster.drop(data_tools.cols_high_correlated, axis=1)\n",
    "    print('     generating categorical db (quantils)...')\n",
    "    categorical_db = numeric_binner(dataset)\n",
    "    categorical_db = categorical_db.drop(data_tools.cols_high_correlated[1:], axis=1)\n",
    "    print('     saving datasets...')\n",
    "    save_data(dataset_cluster, categorical_db, file_path)\n",
    "\n",
    "process_predictive_sets(prefix='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1dcf7a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(data_tools.output_path, 'descriptive_mining', f'{prefix}_descriptive_without_featuring.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4951d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'..\\\\output\\\\descriptive_mining'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53d234f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7c215b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90342442",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a954b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ceba27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ea8810",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90267b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95443207",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003b7f6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
